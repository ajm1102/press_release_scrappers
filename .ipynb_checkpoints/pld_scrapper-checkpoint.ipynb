{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75d2369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "from functions import (get_sector, changes_from_press, \n",
    "                       get_df, normalise_to_index, save_data)\n",
    "columns = ['date', 'press title', 'url', '1d change', 'index_price','norm_price']\n",
    "past_last_date = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68715c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "C:\\Users\\Aidan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "sdfs\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "page_num = 1\n",
    "\n",
    "ticker = 'pld'\n",
    "sector = get_sector(ticker)\n",
    "\n",
    "index_ticker = '^GSPC'\n",
    "\n",
    "stock_hist = get_df(ticker)\n",
    "index_hist = get_df(index_ticker)\n",
    "\n",
    "\n",
    "# create break\n",
    "while True:\n",
    "    print(page_num)\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"}\n",
    "    \n",
    "    s = requests.Session()\n",
    "    html = requests.get(f'https://ir.prologis.com/press-releases?page={page_num}', headers=headers)\n",
    "    \n",
    "    soup = BeautifulSoup(html.content) \n",
    "\n",
    "    articles = soup.find_all('article',attrs={'class':'media'})\n",
    "    if page_num == 0:\n",
    "        first_page = articles\n",
    "    if not articles:        \n",
    "        break\n",
    "    if page_num != 1 and articles == first_page:\n",
    "        break\n",
    "        \n",
    "    for article in articles:\n",
    "        date = article.find('time').text.lstrip()\n",
    "        title = article.find('a').text\n",
    "        url = article.find('a').get('href')\n",
    "        \n",
    "        title = title.lstrip().rstrip()\n",
    "\n",
    "        date = dparser.parse(date,fuzzy=False).date()\n",
    "        \n",
    "        if date < datetime(2014, 1, 1).date():            \n",
    "            past_last_date = True\n",
    "            break        \n",
    "        \n",
    "        pct_change = changes_from_press(stock_hist, date, 1)\n",
    "        \n",
    "        if pct_change != None:\n",
    "            index_price, norm_price = normalise_to_index(pct_change, date, index_hist)\n",
    "        else:\n",
    "            index_price, norm_price = (None, None)\n",
    "\n",
    "        data.append([date, title, url, pct_change, index_price, norm_price])\n",
    "    if past_last_date == True: break\n",
    "    page_num = page_num + 1\n",
    "    \n",
    "save_data(data, columns, ticker, sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872d4e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
